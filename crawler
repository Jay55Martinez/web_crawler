#!/usr/bin/env python3

import argparse
import socket
import ssl
import certifi
import re

DEFAULT_SERVER = "fakebook.khoury.northeastern.edu"
DEFAULT_PORT = 443

class Crawler:
    def __init__(self, args):
        self.debug = True
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password
        self.context = ssl.create_default_context(cafile=certifi.where())
        self.sslsocket = None
        self.cookies = {}
        self.queue = list()
        self.visited = set()
        self.num_secret_flags = 0
        self.csrf_token = None

    def connect(self):
        """Establishes a secure connection to the server."""
        try:
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.sslsocket = self.context.wrap_socket(self.socket, server_hostname=self.server)
            self.sslsocket.connect((self.server, self.port))
        except Exception as e:
            print(f"Error connecting to server: {e}")

    def close(self):
        """Closes the connection."""
        try:
            if self.sslsocket:
                self.sslsocket.close()
        except Exception as e:
            print(f"Error closing the connection: {e}")

    def rec_webpage(self, path="/"):
        """Sends a GET request and retrieves the server's response."""
        try:
            request = (f"GET {path} HTTP/1.1\r\nHost: {self.server}:{self.port}\r\nConnection: close\r\n")
            if self.cookies:
                request += f"Cookie: {self.pass_cookies_responce()}\r\n"
            request += "\r\n"
            self.sslsocket.send(request.encode('ascii'))

            response = b""
            while True:
                chunk = self.sslsocket.recv(4096)
                if not chunk:
                    break
                response += chunk

            return response.decode('utf-8', errors='ignore')
        except Exception as e:
            print(f"Error receiving webpage: {e}")
            return ""

    def extract_next_link(self, response, target_text):
        """Extracts the URL path of a link containing the target text."""
        try:
            pattern = re.compile(r'<a href="([^"]+)">.*?' + re.escape(target_text) + r'.*?</a>', re.IGNORECASE)
            match = pattern.search(response)
            if match:
                return match.group(1)
            return None
        except Exception as e:
            print(f"Error extracting next link: {e}")
            return None

    def goto_next_location(self, response):
        """Extracts the 'Location' header value from an HTTP response."""
        try:
            pattern = re.compile(r'location:\s*([^\r\n]+)', re.IGNORECASE)
            match = pattern.search(response)
            if match:
                return match.group(1).strip()  # Clean any whitespace
            return None
        except Exception as e:
            print(f"Error extracting next location: {e}")
            return None

    def post_login(self):
        """Posts the login form to the server."""
        try:
            request = (f"POST /accounts/login/ HTTP/1.1\r\nHost: {self.server}:{self.port}\r\nContent-Type: application/x-www-form-urlencoded\r\nContent-Length: 200\r\nCookie: {self.pass_cookies_responce()}\r\nConnection: close\r\n\r\nusername={self.username}&password={self.password}&csrfmiddlewaretoken={self.csrf_token}&next=%2Ffakebook%2F\r\n\r\n")
            self.sslsocket.send(request.encode('ascii'))
            response = b""
            while True:
                chunk = self.sslsocket.recv(4096)
                if not chunk:
                    break
                response += chunk
            return response.decode('utf-8', errors='ignore')
        except Exception as e:
            print(f"Error posting login: {e}")
            return ""

    def update_cookies(self, response):
        """Extracts the cookies from the server's response."""
        try:
            pattern = re.compile(r'set-cookie:\s*([^=]+)=([^;]+)', re.IGNORECASE)
            matches = pattern.findall(response)
            for match in matches:
                self.cookies[match[0]] = match[1]
        except Exception as e:
            print(f"Error updating cookies: {e}")

    def extract_flags(self, response):
        """
        Extracts secret flags from the given HTML response.

        Args:
            response (str): HTML content of the webpage.

        Returns:
            list: A list of secret flags found in the response.
        """
        try:
            pattern = re.compile(r"<h3 class='secret_flag' style=\"color:red\">FLAG: ([a-zA-Z0-9]{64})</h3>")
            flags = pattern.findall(response)
            with open("secret_flags", "a") as f:
                for flag in flags:
                    print(flag)
                    f.write(flag + "\n")
                    self.num_secret_flags += 1
            return flags
        except Exception as e:
            print(f"Error extracting flags: {e}")
            return []

    def extract_csrf_token(self, response):
        """Extracts the CSRF token from the HTML response."""
        try:
            pattern = re.compile(r'name="csrfmiddlewaretoken" value="([^"]+)"')
            match = pattern.search(response)
            if match:
                self.csrf_token = match.group(1)
            return None
        except Exception as e:
            print(f"Error extracting CSRF token: {e}")
            return None

    def pass_cookies_responce(self):
        """Returns the cookies in the response format. returns a string"""
        try:
            cookie_responce = ""
            for key, value in self.cookies.items():
                cookie_responce += f"{key}={value};"
            if cookie_responce:
                return cookie_responce[:-1]
        except Exception as e:
            print(f"Error passing cookies response: {e}")
            return ""

    def crawling_logic(self, response):
        """
        Takes the response of the webpage with the friends list and returns all of the links
        To the next friends page.

        Args:
            response (str): html response of the friends page
        """
        try:
            pattern = re.compile(r'<a href="([^"]+)">.*?' r'.*?</a>', re.IGNORECASE)
            matches = pattern.findall(response)
            return matches
        except Exception as e:
            print(f"Error in crawling logic: {e}")
            return []

    def extract_friends(self, response):
        """Extracts names and links from the given HTML response and appends them to self.friends."""
        try:
            # Regex pattern to match <a href="link">Name</a>
            pattern = re.compile(r'<a href="(/fakebook/\d+)/">(.*?)</a>')
            
            # Find all matches of the pattern in the response
            matches = pattern.findall(response)
            
            # Append each name and link to the self.friends dictionary
            for link, name in matches:
                self.queue.append(link)
        except Exception as e:
            print(f"Error extracting friends: {e}")

    def get_friends_friends(self, response):
        """Takes the response of a friends page and returns the friends of the friends"""
        try:
            pattern = re.compile(r'<a href="(/fakebook/\d+/friends/\d+/)">(.*?)', re.IGNORECASE)
            matches = pattern.findall(response)
            
            for link, name in matches:
                self.queue.append(link)
        except Exception as e:
            print(f"Error getting friends of friends: {e}")

    def bfs_crawl(self, initial_response):
        """
        Performs a breadth-first search (BFS) on the friends' network to find secret flags.
        Stops when 5 flags are discovered.
        """
        try:
            # Extract initial friends from the response
            self.extract_friends(initial_response)

            while self.queue and self.num_secret_flags < 5:  # Stop after discovering 5 flags
                # Get the next friend's link from the queue
                current_link = self.queue.pop(0)

                # Skip if already visited
                if current_link in self.visited:
                    continue
                
                self.visited.add(current_link)

                # Fetch and parse the friend's page
                self.connect()
                response = self.rec_webpage(current_link)
                self.get_friends_friends(response)
                self.close()

                # Follow any redirects
                next_link = self.goto_next_location(response)
                if next_link:
                    self.connect()
                    response = self.rec_webpage(next_link)
                    self.get_friends_friends(response)
                    self.close()

                # Extract flags from the current page
                self.extract_flags(response)

                # Add new links to the queue
                self.get_friends_friends(response)
                self.extract_friends(response)
        except Exception as e:
            print(f"Error in BFS crawl: {e}")

    def start_sequence(self):
        """Attempts to log into fakebook returns the first list of friends"""
        try:
            self.connect()
            response = self.rec_webpage()
            self.extract_csrf_token(response)
            self.close()
            self.connect()
            next_link = self.extract_next_link(response, "Fakebook")
            response = self.rec_webpage(next_link)
            next_link = self.goto_next_location(response)
            self.extract_csrf_token(response)
            self.close()
            self.connect()
            response = self.rec_webpage(next_link)
            self.update_cookies(response)
            self.extract_csrf_token(response)
            self.close()
            self.connect()
            response = self.post_login()
            self.update_cookies(response)
            self.close()
            self.connect()
            next_link = self.goto_next_location(response)
            response = self.rec_webpage(next_link)
            self.update_cookies(response)
            self.close()
            return response
        except Exception as e:
            print(f"Error in start sequence: {e}")
            return ""

    def run(self):
        """Main method to connect, fetch a webpage, and close the connection."""
        try:
            friends_page_responce = self.start_sequence()
            
            # Crawling logic
            self.bfs_crawl(friends_page_responce)
        except Exception as e:
            print(f"Error in run method: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Crawl Fakebook")
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()

    crawler = Crawler(args)
    crawler.run()
